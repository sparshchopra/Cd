 # Install required dependencies
!pip install pydicom opencv-python-headless psutil

import numpy as np
import tensorflow as tf
from tensorflow import keras
import random
import copy
import os
import cv2
import pydicom
import psutil
from typing import List, Dict
import sys
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

class MedicalImageCompiler:
    def __init__(self, 
                 num_classes: int,
                 population_size: int = 6,
                 max_generations: int = 3,
                 mutation_rate: float = 0.15):
        self.num_classes = num_classes
        self.population_size = population_size
        self.max_generations = max_generations
        self.mutation_rate = mutation_rate
        
        self.compilation_parameters = {
            'learning_rate': (0.0001, 0.01),
            'batch_size': (8, 16),  # Smaller batches
            'optimizer_type': ['adam'],
            'layer_types': ['conv2d', 'dense'],
            'activation_functions': ['relu']
        }

    def _build_model(self, config: Dict) -> keras.Model:
        model = keras.Sequential()
        model.add(keras.layers.InputLayer(input_shape=(128, 128, 3)))
        
        for layer in config['architecture']:
            if layer['type'] == 'conv2d':
                model.add(keras.layers.Conv2D(
                    filters=layer['filters'],
                    kernel_size=(3, 3),
                    activation=layer['activation']
                ))
                model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
            elif layer['type'] == 'dense':
                model.add(keras.layers.Flatten())
                model.add(keras.layers.Dense(
                    units=layer['units'],
                    activation=layer['activation']
                ))
        
        model.add(keras.layers.Dense(self.num_classes, activation='softmax'))
        
        model.compile(
            optimizer=keras.optimizers.get({
                'class_name': config['optimizer'],
                'config': {'learning_rate': config['learning_rate']}
            }),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    def _generate_individual(self) -> Dict:
        individual = {
            'learning_rate': np.random.uniform(*self.compilation_parameters['learning_rate']),
            'batch_size': np.random.randint(*self.compilation_parameters['batch_size']),
            'optimizer': random.choice(self.compilation_parameters['optimizer_type']),
            'architecture': []
        }
        
        num_layers = np.random.randint(2, 4)
        for _ in range(num_layers):
            layer_type = random.choice(self.compilation_parameters['layer_types'])
            layer = {'type': layer_type}
            
            if layer_type == 'conv2d':
                layer.update({
                    'filters': np.random.choice([32, 64]),
                    'activation': random.choice(self.compilation_parameters['activation_functions'])
                })
            elif layer_type == 'dense':
                layer.update({
                    'units': np.random.choice([128, 256]),
                    'activation': random.choice(self.compilation_parameters['activation_functions'])
                })
                
            individual['architecture'].append(layer)
            
        return individual

    def _mutate_individual(self, individual: Dict) -> Dict:
        mutated = copy.deepcopy(individual)
        
        if random.random() < self.mutation_rate:
            mutated['learning_rate'] = np.random.uniform(*self.compilation_parameters['learning_rate'])
        
        if random.random() < self.mutation_rate:
            mutated['batch_size'] = np.random.randint(*self.compilation_parameters['batch_size'])
            
        if random.random() < self.mutation_rate:
            mutated['optimizer'] = random.choice(self.compilation_parameters['optimizer_type'])
            
        if random.random() < self.mutation_rate and len(mutated['architecture']) > 2:
            del mutated['architecture'][np.random.randint(len(mutated['architecture']))]
                
        if random.random() < self.mutation_rate:
            new_layer = self._generate_individual()['architecture'][0]
            mutated['architecture'].insert(np.random.randint(len(mutated['architecture'])), new_layer)
            
        return mutated

    def _crossover(self, parent1: Dict, parent2: Dict) -> Dict:
        child = {
            'learning_rate': np.mean([parent1['learning_rate'], parent2['learning_rate']]),
            'batch_size': int(np.mean([parent1['batch_size'], parent2['batch_size']])),
            'optimizer': random.choice([parent1['optimizer'], parent2['optimizer']]),
            'architecture': []
        }
        
        min_len = min(len(parent1['architecture']), len(parent2['architecture']))
        for i in range(min_len):
            child['architecture'].append(random.choice([parent1['architecture'][i], parent2['architecture'][i]]))
        
        return child

    def _evaluate_individual(self, individual: Dict, X_val, y_val) -> float:
        model = self._build_model(individual)
        _, val_acc = model.evaluate(X_val, y_val, verbose=0)
        tf.keras.backend.clear_session()  # Clear memory
        return val_acc

    def genetic_optimization(self, X_train, y_train, X_val, y_val) -> Dict:
        population = [self._generate_individual() for _ in range(self.population_size)]
        
        for generation in range(self.max_generations):
            print(f"\nGeneration {generation + 1}/{self.max_generations}")
            fitness = []
            
            for idx, individual in enumerate(population):
                try:
                    model = self._build_model(individual)
                    history = model.fit(
                        X_train, y_train,
                        batch_size=individual['batch_size'],
                        epochs=1,  # Reduced from 2
                        validation_data=(X_val, y_val),
                        verbose=0
                    )
                    val_acc = max(history.history['val_accuracy'])
                    fitness.append((idx, val_acc))
                    print(f"Model {idx + 1}: Val Acc = {val_acc:.4f}")
                    del model  # Free memory
                    tf.keras.backend.clear_session()
                except:
                    fitness.append((idx, 0.0))
                    
            fitness.sort(key=lambda x: x[1], reverse=True)
            selected = [population[i] for i, _ in fitness[:int(self.population_size * 0.3)]]
            
            new_population = selected.copy()  # Keep elites
            while len(new_population) < self.population_size:
                parents = random.choices(selected, k=2)
                child = self._crossover(parents[0], parents[1])
                child = self._mutate_individual(child)
                new_population.append(child)
                
            population = new_population
            
        return max(population, key=lambda x: self._evaluate_individual(x, X_val, y_val))

def load_medical_dataset(data_path: str):
    try:
        abs_path = os.path.abspath(data_path)
        class_labels = sorted([d for d in os.listdir(abs_path) if os.path.isdir(os.path.join(abs_path, d))])
        
        X, y = [], []
        valid_extensions = ('.png', '.jpg', '.jpeg', '.dcm')

        for class_idx, class_name in enumerate(class_labels):
            class_dir = os.path.join(abs_path, class_name)
            image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(valid_extensions)][:200]  # Small subset
            
            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                try:
                    if img_file.lower().endswith('.dcm'):
                        ds = pydicom.dcmread(img_path)
                        img = cv2.normalize(ds.pixel_array.astype(float), None, 0, 255, cv2.NORM_MINMAX)
                        img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_GRAY2RGB)
                    else:
                        img = cv2.imread(img_path)
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    
                    img = cv2.resize(img, (128, 128))
                    X.append(img.astype(np.float16))  # Half precision
                    y.append(class_idx)
                
                except Exception as e:
                    continue

        return np.array(X), keras.utils.to_categorical(y, num_classes=len(class_labels))

    except Exception as e:
        print(f"Dataset loading failed: {str(e)}")
        sys.exit(1)

def monitor_memory():
    process = psutil.Process()
    print(f"Memory usage: {process.memory_info().rss/1024/1024:.2f} MB")
    if tf.config.list_physical_devices('GPU'):
        print(f"GPU Memory: {tf.config.experimental.get_memory_info('GPU:0')['current']/1024/1024:.2f} MB")

if __name__ == "__main__":
    # Configure paths
    base_path = "/content/drive/MyDrive/classifier.v1i.clip"
    train_data_path = os.path.join(base_path, "train")
    val_data_path = os.path.join(base_path, "valid")
    
    # Memory optimization
    tf.config.optimizer.set_jit(True)
    if tf.config.list_physical_devices('GPU'):
        tf.config.experimental.set_memory_growth(
            tf.config.list_physical_devices('GPU')[0], True
        )

    try:
        print("Loading data...")
        X_train, y_train = load_medical_dataset(train_data_path)
        X_val, y_val = load_medical_dataset(val_data_path)
        
        monitor_memory()
        
        compiler = MedicalImageCompiler(
            num_classes=y_train.shape[1],
            population_size=6,
            max_generations=3
        )
        
        print("Starting optimization...")
        best_config = compiler.genetic_optimization(X_train, y_train, X_val, y_val)
        
        best_model = compiler._build_model(best_config)
        best_model.save("/content/drive/MyDrive/optimized_model.h5")
        print("Optimization complete!")
        
    except Exception as e:
        print(f"Error: {str(e)}")
